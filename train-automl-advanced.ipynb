{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aab5e1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T21:22:06.535068Z",
     "start_time": "2024-03-18T21:22:06.476699Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import mercury as mr\n",
    "from supervised.automl import AutoML\n",
    "\n",
    "\n",
    "#imports for Certain Model\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import lux\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "mercury.App",
      "text/html": "<h3>Mercury Application</h3><small>This output won't appear in the web app.</small>",
      "application/mercury+json": "{\n    \"widget\": \"App\",\n    \"title\": \"Certain Clean: Minimal Imputation\",\n    \"description\": \"Train ML pipeline \",\n    \"show_code\": false,\n    \"show_prompt\": false,\n    \"output\": \"app\",\n    \"schedule\": \"\",\n    \"notify\": \"{}\",\n    \"continuous_update\": true,\n    \"static_notebook\": false,\n    \"show_sidebar\": true,\n    \"full_screen\": true,\n    \"allow_download\": true,\n    \"stop_on_error\": false,\n    \"model_id\": \"mercury-app\",\n    \"code_uid\": \"App.0.40.105.1-randfd75a6b2\"\n}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app = mr.App(title=\"Certain Clean: Minimal Imputation\", description=\"Train ML pipeline \")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T21:22:06.606877Z",
     "start_time": "2024-03-18T21:22:06.562156Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "\n",
    "def missing_values_table(df):\n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "\n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "\n",
    "    # Data types of columns\n",
    "    data_types = df.dtypes\n",
    "\n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent, data_types], axis=1)\n",
    "\n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns={0: 'Missing Values', 1: '% of Total Values', 2: 'Data Type'})\n",
    "\n",
    "    # Sort the table by percentage of missing descending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    # Print some summary information\n",
    "    print(\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n",
    "                                                              \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "          \" columns that have missing values.\")\n",
    "\n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "\n",
    "def tryParse(X):\n",
    "    vals = []\n",
    "\n",
    "    if X.shape == (1, 1):\n",
    "        try:\n",
    "            vals.append(float(X.tolist()[0][0]))\n",
    "        except ValueError:\n",
    "            vals.append(0)\n",
    "\n",
    "        return vals\n",
    "\n",
    "    for x in np.squeeze(X.T):\n",
    "        try:\n",
    "            vals.append(float(x))\n",
    "        except ValueError:\n",
    "            vals.append(0)\n",
    "\n",
    "    return vals\n",
    "\n",
    "\n",
    "\n",
    "def manual_categorical_imputation(df, categorical_columns):\n",
    "    df=df.reset_index(drop=True)\n",
    "     # Step 1: Fill categorical missings with \"missing\"\n",
    "    df[categorical_columns] = df[categorical_columns].fillna(\"missing\")\n",
    "    assertion = df.isin(['missing']).any().any()\n",
    "    # Step 2: Use OneHotEncoder\n",
    "    # encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False).set_output(transform=\"pandas\")\n",
    "    # encoded_data = pd.DataFrame(encoder.fit_transform(df[categorical_columns]))\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    # fit and transform color column\n",
    "    one_hot_array = encoder.fit_transform(df[categorical_columns]).toarray()\n",
    "\n",
    "    # create new dataframe from numpy array\n",
    "    encoded_data = pd.DataFrame(one_hot_array, columns=encoder.get_feature_names_out(), index=df.index)\n",
    "\n",
    "    assertion = encoded_data.isin(['missing']).any().any()\n",
    "    # Step 3: Get feature names and identify missing indicator columns\n",
    "    feature_names = encoder.get_feature_names_out()\n",
    "    missing_indicator_cols = [col for col in feature_names if '_missing' in col]\n",
    "    #df = pd.concat([df, encoded_data], axis=1)\n",
    "\n",
    "    # Step 4: Replace original categorical columns with NaN where missing indicator is 1\n",
    "    for categorical_col in categorical_columns:\n",
    "        missing_indicator_col = f\"{categorical_col}_missing\"\n",
    "\n",
    "        if missing_indicator_col in missing_indicator_cols:\n",
    "            mask = (encoded_data[missing_indicator_col] == 1)\n",
    "\n",
    "            # Replace all columns that start with categorical_col with NaN where missing indicator is 1\n",
    "            cols_to_replace = [col for col in encoded_data.columns if col.startswith(categorical_col)]\n",
    "            encoded_data.loc[mask, cols_to_replace] = np.nan\n",
    "            encoded_data.drop(columns=[missing_indicator_col], inplace=True)\n",
    "\n",
    "\n",
    "    df.drop(columns=categorical_columns,inplace=True)\n",
    "    df = pd.concat([df, encoded_data], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_label_with_null(df, column_name):\n",
    "    # Drop rows where the specified column is null\n",
    "    df_cleaned = df.dropna(subset=[column_name])\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "def encoding(test_df):\n",
    "\n",
    "\n",
    "    ohe = OneHotEncoder(\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse_output=False,\n",
    "        # handle_missing=\"ignore\"\n",
    "    )\n",
    "    ohe.fit_transform(test_df)\n",
    "    return test_df\n",
    "\n",
    "def drop_categorical_columns(df,conversion=False,featurize=False):\n",
    "    # Identify categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "    return_df = df\n",
    "\n",
    "    if conversion==True:\n",
    "        # this is to avoid droping int and float mixed type columns since they will be considered objects\n",
    "        for col in categorical_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "        # after those are taken care of we can drop the columns that are still object\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        for col in categorical_columns:\n",
    "            # Find the most common value in the column\n",
    "            most_common_value = df[col].mode().iloc[0]\n",
    "\n",
    "            # Map the non-null column values accordingly\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: 1 if pd.notna(x) and x == most_common_value else (0 if pd.notna(x) else x))\n",
    "\n",
    "        return_df=df.copy()\n",
    "\n",
    "    elif featurize==True:\n",
    "        # this is to avoid droping int and float mixed type columns since they will be considered objects\n",
    "        for col in categorical_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "        # after those are taken care of we can drop the columns that are still object\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        for col in categorical_columns:\n",
    "            if df[col].nunique() > 20:\n",
    "             df.drop(columns=[col],inplace=True)\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "        return_df=manual_categorical_imputation(df,categorical_columns)\n",
    "\n",
    "    else:\n",
    "        #this is to avoid droping int and float mixed type columns since they will be considered objects\n",
    "        for col in categorical_columns:\n",
    "                df[col]=pd.to_numeric(df[col], errors='ignore')\n",
    "        #after those are taken care of we can drop the columns that are still object\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        return_df = df.drop(categorical_columns, axis=1)\n",
    "\n",
    "        # # Drop categorical columns from the DataFrame\n",
    "        # return_df =  df.drop(categorical_columns, axis=1)\n",
    "\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def split_features_labels(df, label_column, task='Regression',situation='mean'):\n",
    "    # Check if the specified label column exists in the DataFrame\n",
    "    if label_column not in df.columns:\n",
    "        print(f\"Label column '{label_column}' not found in the DataFrame.\")\n",
    "        return None, None\n",
    "    df_drop_label= df\n",
    "    # Features (X) are all columns except the specified label column\n",
    "    X = df_drop_label.drop(label_column, axis=1)\n",
    "    if task=='classification':\n",
    "        # Label (y) is the specified column\n",
    "        # Create a new binary column based on the specified midpoint\n",
    "        if len(df_drop_label[label_column].unique())<2:\n",
    "            print(f\"Label column '{label_column}'has only one label\")\n",
    "        elif len(df_drop_label[label_column].unique())>2:\n",
    "            if situation=='mode':\n",
    "                midpoint = df_drop_label[label_column].mode().iloc[0]\n",
    "                df_drop_label.loc[:, label_column] = df_drop_label[label_column].apply(lambda x: 1 if x == midpoint else 0)\n",
    "            elif situation=='median':\n",
    "                midpoint = df_drop_label[label_column].median()\n",
    "                df_drop_label.loc[:, label_column] = df_drop_label[label_column].apply(lambda x: 1 if x > midpoint else 0)\n",
    "            else:\n",
    "                midpoint = df_drop_label[label_column].mean()\n",
    "                df_drop_label.loc[:, label_column] = df_drop_label[label_column].apply(\n",
    "                    lambda x: 1 if x > midpoint else 0)\n",
    "        else:\n",
    "            print(f\"Label column '{label_column}'is PERFECT for Classification\")\n",
    "    else:\n",
    "        pass\n",
    "        #print(f\"Label column '{label_column}'is used for Regression\")\n",
    "\n",
    "    y = df_drop_label[label_column]\n",
    "\n",
    "    return X, y\n",
    "def get_single_value_columns(df):\n",
    "    # Identify columns with only one unique value\n",
    "    single_value_cols = df.columns[df.nunique() == 1].tolist()\n",
    "\n",
    "    return single_value_cols\n",
    "def read_names_file(file_path):\n",
    "    feature_names = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Assuming feature names are listed in lines starting with a capital letter\n",
    "            if re.match(r'^[A-Z]', line):\n",
    "                feature_name = line.split()[0]\n",
    "                feature_names.append(feature_name)\n",
    "\n",
    "    return feature_names\n",
    "def get_Xy(data,label):\n",
    "    X = data.drop(label,axis = 1)\n",
    "    y = data[label]\n",
    "    return X,y\n",
    "\n",
    "\n",
    "def get_simple_imputer_model_classification(df_train, df_test, label):\n",
    "    X_train, y_train=get_Xy(df_train,label)\n",
    "    X_test, y_test=get_Xy(df_test,label)\n",
    "    start_time_s = time.time()\n",
    "    # Get all column names with nulls\n",
    "    columns_with_nulls = X_train.columns[X_train.isnull().any()]\n",
    "\n",
    "    # Simple imputation using mean strategy for each column\n",
    "    meanimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "    # Simple imputation using mean strategy for each column\n",
    "    modeimputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "    for col in columns_with_nulls:\n",
    "        if X_train[col].nunique() > 2:\n",
    "            X_train[col] = meanimputer.fit_transform(X_train[[col]]).flatten()\n",
    "        else:\n",
    "            X_train[col] = modeimputer.fit_transform(X_train[[col]]).flatten()\n",
    "\n",
    "    # Assert that there are no more null values in X_train\n",
    "    assert not X_train.isnull().any().any()\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        alpha=0.0000000001,\n",
    "        max_iter=10000,\n",
    "        fit_intercept=True,\n",
    "        warm_start=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    end_time_s = time.time()\n",
    "    simple_time = end_time_s - start_time_s\n",
    "    return score, simple_time\n",
    "\n",
    "\n",
    "def get_knn_imputer_model_classification(df_train, df_test, label):\n",
    "    X_train, y_train=get_Xy(df_train,label)\n",
    "    X_test, y_test=get_Xy(df_test,label)\n",
    "    start_time_s = time.time()\n",
    "    # Get all column names with nulls\n",
    "    columns_with_nulls = X_train.columns[X_train.isnull().any()]\n",
    "\n",
    "    # Simple imputation using mean strategy for each column\n",
    "    imputer = KNNImputer(missing_values=np.nan)\n",
    "    imputed_X=imputer.fit_transform(X_train)\n",
    "\n",
    "    # Assert that there are no more null values in X_train\n",
    "    assert not pd.DataFrame(imputed_X).isnull().any().any()\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        alpha=0.0000000001,\n",
    "        max_iter=10000,\n",
    "        fit_intercept=True,\n",
    "        warm_start=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "    clf.fit(imputed_X, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    end_time_s = time.time()\n",
    "    simple_time = end_time_s - start_time_s\n",
    "    return score, simple_time\n",
    "\n",
    "def get_naive_imputer_model_classification(df_train, df_test, label):\n",
    "    X_train, y_train = get_Xy(df_train, label)\n",
    "    X_test, y_test = get_Xy(df_test, label)\n",
    "\n",
    "    # Copy X_train and drop rows with null values\n",
    "    X_train_copy = X_train.copy()\n",
    "    X_train_copy.dropna(inplace=True)\n",
    "\n",
    "    # Align y_train with the modified X_train\n",
    "    y_train_aligned = y_train.loc[X_train_copy.index]\n",
    "\n",
    "    # Assert that there are no more null values in X_train\n",
    "    assert not X_train_copy.isnull().any().any()\n",
    "\n",
    "    # Train classifier\n",
    "    start_time = time.time()\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        alpha=0.0000000001,\n",
    "        max_iter=10000,\n",
    "        fit_intercept=True,\n",
    "        warm_start=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "    clf.fit(X_train_copy, y_train_aligned)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    return score, execution_time\n",
    "\n",
    "\n",
    "def get_naive_imputer_model_regression(df_train, df_test, label):\n",
    "    X_train, y_train=get_Xy(df_train,label)\n",
    "    X_test, y_test=get_Xy(df_test,label)\n",
    "    start_time_s = time.time()\n",
    "    # Drop rows with null values in the copy of X_train\n",
    "    X_train.dropna(inplace=True)\n",
    "\n",
    "    # Now X_train is a copy and not a view, and modifications won't raise the warning\n",
    "\n",
    "    # Align y_train with the modified X_train\n",
    "    y_train = y_train.iloc[X_train.index]\n",
    "\n",
    "    # Assert that there are no more null values in X_train\n",
    "    assert not X_train.isnull().any().any()\n",
    "\n",
    "\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        alpha=0.0000000001,\n",
    "        max_iter=10000,\n",
    "        fit_intercept=True,\n",
    "        warm_start=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = mean_squared_error(y_test, y_pred)\n",
    "    # score = clf.score(X_test, y_test)\n",
    "\n",
    "    end_time_s = time.time()\n",
    "    naive_time = end_time_s - start_time_s\n",
    "    return score, naive_time\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T21:22:06.856853Z",
     "start_time": "2024-03-18T21:22:06.851396Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def check_certain_model(X_train, y_train, X_test, y_test):\n",
    "    res = True\n",
    "\n",
    "    # Convert X_train and y_train to NumPy arrays\n",
    "    X_train = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "    y_train = y_train.values if isinstance(y_train, pd.DataFrame) else y_train\n",
    "\n",
    "    # Find indices of columns with missing values in X_train\n",
    "    missing_columns_indices = np.where(pd.DataFrame(X_train).isnull().any(axis=0))[0]\n",
    "\n",
    "    # Find rows with missing values in X_train\n",
    "    missing_rows_indices = np.where(pd.DataFrame(X_train).isnull().any(axis=1))[0]\n",
    "\n",
    "    # Record the rows with missing values and their corresponding y_train values\n",
    "    X_train_missing_rows = X_train[missing_rows_indices]\n",
    "    y_train_missing_rows = y_train[missing_rows_indices]\n",
    "\n",
    "    # Remove rows with missing values from X_train and corresponding labels from y_train\n",
    "    X_train_complete = np.delete(X_train, missing_rows_indices, axis=0)\n",
    "    y_train_complete = np.delete(y_train, missing_rows_indices, axis=0)\n",
    "    # print(X_train_complete.shape)\n",
    "    # Create and train the SVM model using SGDClassifier\n",
    "    svm_model = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        alpha=0.0000000001,\n",
    "        max_iter=10000,\n",
    "        fit_intercept=True,\n",
    "        warm_start=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # Train the model on the data without missing values\n",
    "    svm_model.fit(X_train_complete, y_train_complete)\n",
    "\n",
    "    # Extract the feature weights (model parameters)\n",
    "    feature_weights = svm_model.coef_[0]\n",
    "\n",
    "    # Check if the absolute value of feature_weights[i] is small enough for all i with missing columns\n",
    "    for i in missing_columns_indices:\n",
    "        if abs(feature_weights[i]) >= 1e-3:\n",
    "            res = False\n",
    "            # print(\"weight\", feature_weights[i])\n",
    "            break\n",
    "            # Return False as soon as a condition is not met\n",
    "\n",
    "    # Check the condition for all rows in X_train_missing_rows\n",
    "    for i in range(len(X_train_missing_rows)):\n",
    "        row = X_train_missing_rows[i]\n",
    "        label = y_train_missing_rows[i]\n",
    "        dot_product = np.sum(row[~np.isnan(row)] * feature_weights[~np.isnan(row)])\n",
    "        if label * dot_product <= 1:\n",
    "            # print(\"dot product\", label * dot_product)\n",
    "            res = False\n",
    "            break\n",
    "            # Return False if the condition is not met for any row\n",
    "    if res:\n",
    "        cm_score = svm_model.score(X_test, y_test)\n",
    "    else:\n",
    "        cm_score=0.000000000001\n",
    "    # If all conditions are met, return True\n",
    "    return res, cm_score\n",
    "\n",
    "\n",
    "\n",
    "def get_Xy(data, label):\n",
    "    X = data.drop(label, axis=1)\n",
    "    y = data[label]\n",
    "    return X, y\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T21:22:06.972152Z",
     "start_time": "2024-03-18T21:22:06.967826Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b576b07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T21:22:07.174773Z",
     "start_time": "2024-03-18T21:22:07.166450Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def certain_clean_main(df, label):\n",
    "    X, y = get_Xy(df, label)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    df_train = pd.concat([X_train, Y_train], axis=1)\n",
    "    df_test = pd.concat([X_test, Y_test], axis=1)\n",
    "\n",
    "    df_test.dropna(inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X_test = df_test.iloc[:, :-1]\n",
    "    y_test = df_test.iloc[:, -1]\n",
    "\n",
    "    total_examples = len(X_train)\n",
    "    missing_values_per_row = X_train.isnull().sum(axis=1)\n",
    "    rows_with_missing_values = len(missing_values_per_row[missing_values_per_row > 0])\n",
    "    missing_factor = rows_with_missing_values / total_examples\n",
    "\n",
    "    start_time = time.time()\n",
    "    result, CM_score = check_certain_model(X_train.values, Y_train.values, X_test.values, y_test.values)\n",
    "    end_time = time.time()\n",
    "    CM_time = end_time - start_time\n",
    "\n",
    "    results_data = []\n",
    "\n",
    "    results_data.append({'Metric': 'Number of Rows with missing values', 'Value': rows_with_missing_values})\n",
    "    results_data.append({'Metric': 'Missing Factor', 'Value': missing_factor})\n",
    "    results_data.append({'Metric': 'Running Time (CM)', 'Value': CM_time})\n",
    "    results_data.append({'Metric': 'Accuracy (CM)', 'Value': CM_score})\n",
    "\n",
    "    simple_imputer_score, simpler_imputer_time = get_simple_imputer_model_classification(\n",
    "        df_train, df_test, label\n",
    "    )\n",
    "\n",
    "    knn_imputer_score, knn_imputer_time = get_knn_imputer_model_classification(\n",
    "        df_train, df_test, label\n",
    "    )\n",
    "\n",
    "    naive_imputer_score, naive_imputer_time = get_naive_imputer_model_classification(\n",
    "        df_train, df_test, label\n",
    "    )\n",
    "\n",
    "    results_data.append({'Metric': 'Accuracy (KNN)', 'Value': knn_imputer_score})\n",
    "    results_data.append({'Metric': 'Running Time (KNN)', 'Value': knn_imputer_time})\n",
    "    results_data.append({'Metric': 'Accuracy (MI)', 'Value': simple_imputer_score})\n",
    "    results_data.append({'Metric': 'Running Time (MI)', 'Value': simpler_imputer_time})\n",
    "    results_data.append({'Metric': 'Accuracy (NI)', 'Value': naive_imputer_score})\n",
    "    results_data.append({'Metric': 'Running Time (NI)', 'Value': naive_imputer_time})\n",
    "\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "\n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T21:22:07.361790Z",
     "start_time": "2024-03-18T21:22:07.355226Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "6d62fe00",
   "metadata": {},
   "source": [
    "# Training Machine Learning Models with CertainClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aafac626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T21:22:07.724632Z",
     "start_time": "2024-03-18T21:22:07.708070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5904e9a0f0ea4580a988971a9bdf9336"
      },
      "application/mercury+json": "{\n    \"widget\": \"File\",\n    \"max_file_size\": \"1MB\",\n    \"label\": \"Upload CSV with training data\",\n    \"model_id\": \"5904e9a0f0ea4580a988971a9bdf9336\",\n    \"code_uid\": \"File.0.40.74.1-rand3de053bf\",\n    \"disabled\": false,\n    \"hidden\": false\n}",
      "text/plain": "mercury.File"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_file = mr.File(label=\"Upload CSV with training data\", max_file_size=\"1MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb3cfa8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T21:22:07.902095Z",
     "start_time": "2024-03-18T21:22:07.899143Z"
    }
   },
   "outputs": [
    {
     "ename": "StopExecution",
     "evalue": "",
     "output_type": "error",
     "traceback": null
    }
   ],
   "source": [
    "if data_file.filepath is None:\n",
    "    mr.Stop()\n",
    "else:\n",
    "    df = pd.read_csv(data_file.filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fbf6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = mr.MultiSelect(label=\"Input features\", value=list(df.columns)[:-1], \n",
    "                           choices=list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d64072",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_column = mr.Select(label=\"Target\", value=list(df.columns)[-1], choices=list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if x_columns.value is None or len(x_columns.value) == 0 or y_column.value is None:\n",
    "    print(\"Please select input features and target column\")\n",
    "    mr.Stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mr.Note(\"#### Prepare data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adbefad",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_features = mr.Checkbox(label=\"Construct Golden Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c829fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selection = mr.Checkbox(label=\"Features Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4cc96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mr.Note(\"#### Algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad21129",
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = [\"SVM\", \"Linear\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = mr.MultiSelect(label=\"Algorithms\", value=algos, choices=algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b85b4124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T20:15:45.529033Z",
     "start_time": "2024-03-18T20:15:45.510624Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_values_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmissing_values_table\u001B[49m(df)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'missing_values_table' is not defined"
     ]
    }
   ],
   "source": [
    "mr.Markdown(\"### Missing Value Diagnostics\")\n",
    "%matplotlib inline\n",
    "missing_values_table(df)\n",
    "msno.matrix(df.sample(250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd4a21b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78506f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_models = mr.Checkbox(label=\"Stack Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble = mr.Checkbox(label=\"Train Ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052df3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ff2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = mr.Note(\"#### Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = mr.Numeric(label=\"Number of Folds\", value=5, min=2, max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f103cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = mr.Checkbox(label=\"Suffle Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d234a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratify = mr.Checkbox(label=\"Stratify Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = mr.Select(label=\"Evaluation Metric\", value=\"auto\", \n",
    "                           choices=[\"auto\", \"logloss\", \"f1\", \"average_precision\",\n",
    "                                    \"accuracy\", \"rmse\", \"mse\", \"mae\", \"r2\",\n",
    "                                    \"mape\", \"spearman\", \"pearson\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc779217",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_limit = mr.Select(label=\"Time Limit (seconds)\", value=\"60\", choices=[\"60\", \"120\", \"240\", \"300\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training = mr.Button(label=\"Start Training\", style=\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = mr.OutputDir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = AutoML(mode=\"Compete\",\n",
    "                algorithms=algorithms.value,\n",
    "                train_ensemble=train_ensemble.value,\n",
    "                stack_models=stack_models.value,\n",
    "                golden_features=golden_features.value,\n",
    "                features_selection=features_selection.value,\n",
    "                validation_strategy={\n",
    "                    \"validation_type\": \"kfold\",\n",
    "                    \"k_folds\": int(folds.value),\n",
    "                    \"shuffle\": shuffle.value,\n",
    "                    \"stratify\": stratify.value,\n",
    "                    \"random_seed\": 123\n",
    "                },\n",
    "                eval_metric=eval_metric.value,\n",
    "                total_time_limit=int(time_limit.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79246d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if start_training.clicked:\n",
    "    mr.Markdown(\"### CertainClean Training Results\")\n",
    "    mr.NumberBox(data=123, title=\"Large number\")\n",
    "    output_csv=certain_clean_main(df, 'Potability')\n",
    "    # automl.fit(df[x_columns.value], df[y_column.value])\n",
    "    pd.DataFrame(output_csv).to_csv(output_dir.path+\"/certain_clean_results.csv\", index=False)\n",
    "    # output_filename = os.path.join(output_dir.path, \"certain_model_results\")\n",
    "    # shutil.make_archive(output_filename, 'zip', automl._results_path)\n",
    "    # print the output\n",
    "    mr.Confetti()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if output_csv is None:\n",
    "    mr.Stop()\n",
    "\n",
    "mr.NumberBox([\n",
    "        mr.NumberBox(data=int(output_csv['Value'][0]), title=\"Total missing values\"),\n",
    "        mr.NumberBox(data=int(output_csv['Value'][1]*100), title=\"Missing Factor\"),\n",
    "        mr.NumberBox(data=int(output_csv['Value'][2]), title=\"Certain Model Running Time\"),\n",
    "        mr.NumberBox(data=int(output_csv['Value'][3]*100), title=\"Certain Model Accuracy\"),\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eefe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if automl._best_model is None:\n",
    "    mr.Stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcfbc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202bccd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
