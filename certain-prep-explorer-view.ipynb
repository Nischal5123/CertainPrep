{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aab5e1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T23:13:15.358611Z",
     "start_time": "2024-03-18T23:13:14.331734Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import mercury as mr\n",
    "from supervised.automl import AutoML\n",
    "\n",
    "\n",
    "#imports for Certain Model\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "import lux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "mercury.App",
      "text/html": "<h3>Mercury Application</h3><small>This output won't appear in the web app.</small>",
      "application/mercury+json": "{\n    \"widget\": \"App\",\n    \"title\": \"Data Explorer View\",\n    \"description\": \"Understand your Dataset \",\n    \"show_code\": false,\n    \"show_prompt\": false,\n    \"output\": \"app\",\n    \"schedule\": \"\",\n    \"notify\": \"{}\",\n    \"continuous_update\": true,\n    \"static_notebook\": false,\n    \"show_sidebar\": true,\n    \"full_screen\": true,\n    \"allow_download\": true,\n    \"stop_on_error\": false,\n    \"model_id\": \"mercury-app\",\n    \"code_uid\": \"App.0.40.105.1-randed92f02d\"\n}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app = mr.App(title=\"Data Explorer View\", description=\"Understand your Dataset \", show_sidebar='False')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T23:13:15.575734Z",
     "start_time": "2024-03-18T23:13:15.359412Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "\n",
    "def missing_values_table(df):\n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "\n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "\n",
    "    # Data types of columns\n",
    "    data_types = df.dtypes\n",
    "\n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent, data_types], axis=1)\n",
    "\n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns={0: 'Missing Values', 1: '% of Total Values', 2: 'Data Type'})\n",
    "\n",
    "    # Sort the table by percentage of missing descending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    # Print some summary information\n",
    "    print(\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n",
    "                                                              \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "          \" columns that have missing values.\")\n",
    "\n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "\n",
    "def tryParse(X):\n",
    "    vals = []\n",
    "\n",
    "    if X.shape == (1, 1):\n",
    "        try:\n",
    "            vals.append(float(X.tolist()[0][0]))\n",
    "        except ValueError:\n",
    "            vals.append(0)\n",
    "\n",
    "        return vals\n",
    "\n",
    "    for x in np.squeeze(X.T):\n",
    "        try:\n",
    "            vals.append(float(x))\n",
    "        except ValueError:\n",
    "            vals.append(0)\n",
    "\n",
    "    return vals\n",
    "\n",
    "\n",
    "\n",
    "def manual_categorical_imputation(df, categorical_columns):\n",
    "    df=df.reset_index(drop=True)\n",
    "     # Step 1: Fill categorical missings with \"missing\"\n",
    "    df[categorical_columns] = df[categorical_columns].fillna(\"missing\")\n",
    "    assertion = df.isin(['missing']).any().any()\n",
    "    # Step 2: Use OneHotEncoder\n",
    "    # encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False).set_output(transform=\"pandas\")\n",
    "    # encoded_data = pd.DataFrame(encoder.fit_transform(df[categorical_columns]))\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    # fit and transform color column\n",
    "    one_hot_array = encoder.fit_transform(df[categorical_columns]).toarray()\n",
    "\n",
    "    # create new dataframe from numpy array\n",
    "    encoded_data = pd.DataFrame(one_hot_array, columns=encoder.get_feature_names_out(), index=df.index)\n",
    "\n",
    "    assertion = encoded_data.isin(['missing']).any().any()\n",
    "    # Step 3: Get feature names and identify missing indicator columns\n",
    "    feature_names = encoder.get_feature_names_out()\n",
    "    missing_indicator_cols = [col for col in feature_names if '_missing' in col]\n",
    "    #df = pd.concat([df, encoded_data], axis=1)\n",
    "\n",
    "    # Step 4: Replace original categorical columns with NaN where missing indicator is 1\n",
    "    for categorical_col in categorical_columns:\n",
    "        missing_indicator_col = f\"{categorical_col}_missing\"\n",
    "\n",
    "        if missing_indicator_col in missing_indicator_cols:\n",
    "            mask = (encoded_data[missing_indicator_col] == 1)\n",
    "\n",
    "            # Replace all columns that start with categorical_col with NaN where missing indicator is 1\n",
    "            cols_to_replace = [col for col in encoded_data.columns if col.startswith(categorical_col)]\n",
    "            encoded_data.loc[mask, cols_to_replace] = np.nan\n",
    "            encoded_data.drop(columns=[missing_indicator_col], inplace=True)\n",
    "\n",
    "\n",
    "    df.drop(columns=categorical_columns,inplace=True)\n",
    "    df = pd.concat([df, encoded_data], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_label_with_null(df, column_name):\n",
    "    # Drop rows where the specified column is null\n",
    "    df_cleaned = df.dropna(subset=[column_name])\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "def encoding(test_df):\n",
    "\n",
    "\n",
    "    ohe = OneHotEncoder(\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse_output=False,\n",
    "        # handle_missing=\"ignore\"\n",
    "    )\n",
    "    ohe.fit_transform(test_df)\n",
    "    return test_df\n",
    "\n",
    "def drop_categorical_columns(df,conversion=False,featurize=False):\n",
    "    # Identify categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "    return_df = df\n",
    "\n",
    "    if conversion==True:\n",
    "        # this is to avoid droping int and float mixed type columns since they will be considered objects\n",
    "        for col in categorical_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "        # after those are taken care of we can drop the columns that are still object\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        for col in categorical_columns:\n",
    "            # Find the most common value in the column\n",
    "            most_common_value = df[col].mode().iloc[0]\n",
    "\n",
    "            # Map the non-null column values accordingly\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: 1 if pd.notna(x) and x == most_common_value else (0 if pd.notna(x) else x))\n",
    "\n",
    "        return_df=df.copy()\n",
    "\n",
    "    elif featurize==True:\n",
    "        # this is to avoid droping int and float mixed type columns since they will be considered objects\n",
    "        for col in categorical_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "        # after those are taken care of we can drop the columns that are still object\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        for col in categorical_columns:\n",
    "            if df[col].nunique() > 20:\n",
    "             df.drop(columns=[col],inplace=True)\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "        return_df=manual_categorical_imputation(df,categorical_columns)\n",
    "\n",
    "    else:\n",
    "        #this is to avoid droping int and float mixed type columns since they will be considered objects\n",
    "        for col in categorical_columns:\n",
    "                df[col]=pd.to_numeric(df[col], errors='ignore')\n",
    "        #after those are taken care of we can drop the columns that are still object\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        return_df = df.drop(categorical_columns, axis=1)\n",
    "\n",
    "        # # Drop categorical columns from the DataFrame\n",
    "        # return_df =  df.drop(categorical_columns, axis=1)\n",
    "\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def split_features_labels(df, label_column, task='Regression',situation='mean'):\n",
    "    # Check if the specified label column exists in the DataFrame\n",
    "    if label_column not in df.columns:\n",
    "        print(f\"Label column '{label_column}' not found in the DataFrame.\")\n",
    "        return None, None\n",
    "    df_drop_label= df\n",
    "    # Features (X) are all columns except the specified label column\n",
    "    X = df_drop_label.drop(label_column, axis=1)\n",
    "    if task=='classification':\n",
    "        # Label (y) is the specified column\n",
    "        # Create a new binary column based on the specified midpoint\n",
    "        if len(df_drop_label[label_column].unique())<2:\n",
    "            print(f\"Label column '{label_column}'has only one label\")\n",
    "        elif len(df_drop_label[label_column].unique())>2:\n",
    "            if situation=='mode':\n",
    "                midpoint = df_drop_label[label_column].mode().iloc[0]\n",
    "                df_drop_label.loc[:, label_column] = df_drop_label[label_column].apply(lambda x: 1 if x == midpoint else 0)\n",
    "            elif situation=='median':\n",
    "                midpoint = df_drop_label[label_column].median()\n",
    "                df_drop_label.loc[:, label_column] = df_drop_label[label_column].apply(lambda x: 1 if x > midpoint else 0)\n",
    "            else:\n",
    "                midpoint = df_drop_label[label_column].mean()\n",
    "                df_drop_label.loc[:, label_column] = df_drop_label[label_column].apply(\n",
    "                    lambda x: 1 if x > midpoint else 0)\n",
    "        else:\n",
    "            print(f\"Label column '{label_column}'is PERFECT for Classification\")\n",
    "    else:\n",
    "        pass\n",
    "        #print(f\"Label column '{label_column}'is used for Regression\")\n",
    "\n",
    "    y = df_drop_label[label_column]\n",
    "\n",
    "    return X, y\n",
    "def get_single_value_columns(df):\n",
    "    # Identify columns with only one unique value\n",
    "    single_value_cols = df.columns[df.nunique() == 1].tolist()\n",
    "\n",
    "    return single_value_cols\n",
    "def read_names_file(file_path):\n",
    "    feature_names = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Assuming feature names are listed in lines starting with a capital letter\n",
    "            if re.match(r'^[A-Z]', line):\n",
    "                feature_name = line.split()[0]\n",
    "                feature_names.append(feature_name)\n",
    "\n",
    "    return feature_names\n",
    "def get_Xy(data,label):\n",
    "    X = data.drop(label,axis = 1)\n",
    "    y = data[label]\n",
    "    return X,y\n",
    "\n",
    "\n",
    "def get_simple_imputer_model_classification(df_train, df_test, label):\n",
    "    X_train, y_train=get_Xy(df_train,label)\n",
    "    X_test, y_test=get_Xy(df_test,label)\n",
    "    start_time_s = time.time()\n",
    "    # Get all column names with nulls\n",
    "    columns_with_nulls = X_train.columns[X_train.isnull().any()]\n",
    "\n",
    "    # Simple imputation using mean strategy for each column\n",
    "    meanimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "    # Simple imputation using mean strategy for each column\n",
    "    modeimputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "    for col in columns_with_nulls:\n",
    "        if X_train[col].nunique() > 2:\n",
    "            X_train[col] = meanimputer.fit_transform(X_train[[col]]).flatten()\n",
    "        else:\n",
    "            X_train[col] = modeimputer.fit_transform(X_train[[col]]).flatten()\n",
    "\n",
    "    # Assert that there are no more null values in X_train\n",
    "    assert not X_train.isnull().any().any()\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        alpha=0.0000000001,\n",
    "        max_iter=10000,\n",
    "        fit_intercept=True,\n",
    "        warm_start=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    end_time_s = time.time()\n",
    "    simple_time = end_time_s - start_time_s\n",
    "    return score, simple_time\n",
    "\n",
    "\n",
    "def get_knn_imputer_model_classification(df_train, df_test, label):\n",
    "    X_train, y_train=get_Xy(df_train,label)\n",
    "    X_test, y_test=get_Xy(df_test,label)\n",
    "    start_time_s = time.time()\n",
    "    # Get all column names with nulls\n",
    "    columns_with_nulls = X_train.columns[X_train.isnull().any()]\n",
    "\n",
    "    # Simple imputation using mean strategy for each column\n",
    "    imputer = KNNImputer(missing_values=np.nan)\n",
    "    imputed_X=imputer.fit_transform(X_train)\n",
    "\n",
    "    # Assert that there are no more null values in X_train\n",
    "    assert not pd.DataFrame(imputed_X).isnull().any().any()\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        alpha=0.0000000001,\n",
    "        max_iter=10000,\n",
    "        fit_intercept=True,\n",
    "        warm_start=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "    clf.fit(imputed_X, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    end_time_s = time.time()\n",
    "    simple_time = end_time_s - start_time_s\n",
    "    return score, simple_time\n",
    "\n",
    "def get_naive_imputer_model_classification(df_train, df_test, label):\n",
    "    X_train, y_train = get_Xy(df_train, label)\n",
    "    X_test, y_test = get_Xy(df_test, label)\n",
    "\n",
    "    # Copy X_train and drop rows with null values\n",
    "    X_train_copy = X_train.copy()\n",
    "    X_train_copy.dropna(inplace=True)\n",
    "\n",
    "    # Align y_train with the modified X_train\n",
    "    y_train_aligned = y_train.loc[X_train_copy.index]\n",
    "\n",
    "    # Assert that there are no more null values in X_train\n",
    "    assert not X_train_copy.isnull().any().any()\n",
    "\n",
    "    # Train classifier\n",
    "    start_time = time.time()\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        alpha=0.0000000001,\n",
    "        max_iter=10000,\n",
    "        fit_intercept=True,\n",
    "        warm_start=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "    clf.fit(X_train_copy, y_train_aligned)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    return score, execution_time\n",
    "\n",
    "\n",
    "def get_naive_imputer_model_regression(df_train, df_test, label):\n",
    "    X_train, y_train=get_Xy(df_train,label)\n",
    "    X_test, y_test=get_Xy(df_test,label)\n",
    "    start_time_s = time.time()\n",
    "    # Drop rows with null values in the copy of X_train\n",
    "    X_train.dropna(inplace=True)\n",
    "\n",
    "    # Now X_train is a copy and not a view, and modifications won't raise the warning\n",
    "\n",
    "    # Align y_train with the modified X_train\n",
    "    y_train = y_train.iloc[X_train.index]\n",
    "\n",
    "    # Assert that there are no more null values in X_train\n",
    "    assert not X_train.isnull().any().any()\n",
    "\n",
    "\n",
    "\n",
    "    clf = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        alpha=0.0000000001,\n",
    "        max_iter=10000,\n",
    "        fit_intercept=True,\n",
    "        warm_start=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = mean_squared_error(y_test, y_pred)\n",
    "    # score = clf.score(X_test, y_test)\n",
    "\n",
    "    end_time_s = time.time()\n",
    "    naive_time = end_time_s - start_time_s\n",
    "    return score, naive_time\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T22:11:57.383432Z",
     "start_time": "2024-03-18T22:11:57.374690Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def check_certain_model(X_train, y_train, X_test, y_test):\n",
    "    res = True\n",
    "\n",
    "    # Convert X_train and y_train to NumPy arrays\n",
    "    X_train = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "    y_train = y_train.values if isinstance(y_train, pd.DataFrame) else y_train\n",
    "\n",
    "    # Find indices of columns with missing values in X_train\n",
    "    missing_columns_indices = np.where(pd.DataFrame(X_train).isnull().any(axis=0))[0]\n",
    "\n",
    "    # Find rows with missing values in X_train\n",
    "    missing_rows_indices = np.where(pd.DataFrame(X_train).isnull().any(axis=1))[0]\n",
    "\n",
    "    # Record the rows with missing values and their corresponding y_train values\n",
    "    X_train_missing_rows = X_train[missing_rows_indices]\n",
    "    y_train_missing_rows = y_train[missing_rows_indices]\n",
    "\n",
    "    # Remove rows with missing values from X_train and corresponding labels from y_train\n",
    "    X_train_complete = np.delete(X_train, missing_rows_indices, axis=0)\n",
    "    y_train_complete = np.delete(y_train, missing_rows_indices, axis=0)\n",
    "    # print(X_train_complete.shape)\n",
    "    # Create and train the SVM model using SGDClassifier\n",
    "    svm_model = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        alpha=0.0000000001,\n",
    "        max_iter=10000,\n",
    "        fit_intercept=True,\n",
    "        warm_start=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # Train the model on the data without missing values\n",
    "    svm_model.fit(X_train_complete, y_train_complete)\n",
    "\n",
    "    # Extract the feature weights (model parameters)\n",
    "    feature_weights = svm_model.coef_[0]\n",
    "\n",
    "    # Check if the absolute value of feature_weights[i] is small enough for all i with missing columns\n",
    "    for i in missing_columns_indices:\n",
    "        if abs(feature_weights[i]) >= 1e-3:\n",
    "            res = False\n",
    "            # print(\"weight\", feature_weights[i])\n",
    "            break\n",
    "            # Return False as soon as a condition is not met\n",
    "\n",
    "    # Check the condition for all rows in X_train_missing_rows\n",
    "    for i in range(len(X_train_missing_rows)):\n",
    "        row = X_train_missing_rows[i]\n",
    "        label = y_train_missing_rows[i]\n",
    "        dot_product = np.sum(row[~np.isnan(row)] * feature_weights[~np.isnan(row)])\n",
    "        if label * dot_product <= 1:\n",
    "            # print(\"dot product\", label * dot_product)\n",
    "            res = False\n",
    "            break\n",
    "            # Return False if the condition is not met for any row\n",
    "    if res:\n",
    "        cm_score = svm_model.score(X_test, y_test)\n",
    "    else:\n",
    "        cm_score=0.000000000001\n",
    "    # If all conditions are met, return True\n",
    "    return res, cm_score\n",
    "\n",
    "\n",
    "\n",
    "def get_Xy(data, label):\n",
    "    X = data.drop(label, axis=1)\n",
    "    y = data[label]\n",
    "    return X, y\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T22:11:57.400096Z",
     "start_time": "2024-03-18T22:11:57.384155Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b576b07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T22:11:57.400301Z",
     "start_time": "2024-03-18T22:11:57.387182Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def certain_clean_main(df, label):\n",
    "    X, y = get_Xy(df, label)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    df_train = pd.concat([X_train, Y_train], axis=1)\n",
    "    df_test = pd.concat([X_test, Y_test], axis=1)\n",
    "\n",
    "    df_test.dropna(inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X_test = df_test.iloc[:, :-1]\n",
    "    y_test = df_test.iloc[:, -1]\n",
    "\n",
    "    total_examples = len(X_train)\n",
    "    missing_values_per_row = X_train.isnull().sum(axis=1)\n",
    "    rows_with_missing_values = len(missing_values_per_row[missing_values_per_row > 0])\n",
    "    missing_factor = rows_with_missing_values / total_examples\n",
    "\n",
    "    start_time = time.time()\n",
    "    result, CM_score = check_certain_model(X_train.values, Y_train.values, X_test.values, y_test.values)\n",
    "    end_time = time.time()\n",
    "    CM_time = end_time - start_time\n",
    "\n",
    "    results_data = []\n",
    "\n",
    "    results_data.append({'Metric': 'Number of Rows with missing values', 'Value': rows_with_missing_values})\n",
    "    results_data.append({'Metric': 'Missing Factor', 'Value': missing_factor})\n",
    "    results_data.append({'Metric': 'Running Time (CM)', 'Value': CM_time})\n",
    "    results_data.append({'Metric': 'Accuracy (CM)', 'Value': CM_score})\n",
    "\n",
    "    simple_imputer_score, simpler_imputer_time = get_simple_imputer_model_classification(\n",
    "        df_train, df_test, label\n",
    "    )\n",
    "\n",
    "    knn_imputer_score, knn_imputer_time = get_knn_imputer_model_classification(\n",
    "        df_train, df_test, label\n",
    "    )\n",
    "\n",
    "    naive_imputer_score, naive_imputer_time = get_naive_imputer_model_classification(\n",
    "        df_train, df_test, label\n",
    "    )\n",
    "\n",
    "    results_data.append({'Metric': 'Accuracy (KNN)', 'Value': knn_imputer_score})\n",
    "    results_data.append({'Metric': 'Running Time (KNN)', 'Value': knn_imputer_time})\n",
    "    results_data.append({'Metric': 'Accuracy (MI)', 'Value': simple_imputer_score})\n",
    "    results_data.append({'Metric': 'Running Time (MI)', 'Value': simpler_imputer_time})\n",
    "    results_data.append({'Metric': 'Accuracy (NI)', 'Value': naive_imputer_score})\n",
    "    results_data.append({'Metric': 'Running Time (NI)', 'Value': naive_imputer_time})\n",
    "\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "\n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T22:11:57.521020Z",
     "start_time": "2024-03-18T22:11:57.515169Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "6d62fe00",
   "metadata": {},
   "source": [
    "# Explore the Data with CertainPrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aafac626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T22:11:57.986005Z",
     "start_time": "2024-03-18T22:11:57.966938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4db6db28a03143dca8b6385c97576362"
      },
      "application/mercury+json": "{\n    \"widget\": \"File\",\n    \"max_file_size\": \"1MB\",\n    \"label\": \"Upload CSV with training data\",\n    \"model_id\": \"4db6db28a03143dca8b6385c97576362\",\n    \"code_uid\": \"File.0.40.74.1-rand2a01b96e\",\n    \"disabled\": false,\n    \"hidden\": false\n}",
      "text/plain": "mercury.File"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = mr.OutputDir()\n",
    "#data_file_path = output_dir.path + \"/data.csv\"\n",
    "data_file_path='/Users/aryal/Desktop/VLDB-Demo/Final-Datasets/water_potability.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def exploratory_data_analysis(df):\n",
    "    # Display quick view of training data\n",
    "    mr.Markdown(\"### Quick-View of Training data\")\n",
    "    print(df.head(10))\n",
    "\n",
    "    # Summary statistics\n",
    "    mr.Markdown(\"### Summary Statistics\")\n",
    "    mr.Markdown(\"```\")\n",
    "    mr.Markdown(df.describe().to_markdown())\n",
    "    mr.Markdown(\"```\")\n",
    "\n",
    "    # Data types\n",
    "    mr.Markdown(\"### Data Types\")\n",
    "    mr.Markdown(\"```\")\n",
    "    mr.Markdown(df.dtypes.to_markdown())\n",
    "    mr.Markdown(\"```\")\n",
    "\n",
    "    # Missing values\n",
    "    mr.Markdown(\"### Missing Values\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() == 0:\n",
    "        mr.Markdown(\"No missing values found.\")\n",
    "    else:\n",
    "        mr.Markdown(\"```\")\n",
    "        mr.Markdown(missing_values[missing_values > 0].to_markdown())\n",
    "        mr.Markdown(\"```\")\n",
    "\n",
    "    # Visualizations\n",
    "    mr.Markdown(\"### Visualizations\")\n",
    "\n",
    "    # Pairplot\n",
    "    sns.pairplot(df)\n",
    "    plt.title(\"Pairplot of Training Data\")\n",
    "    plt.savefig(\"pairplot.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Heatmap of Training Data\")\n",
    "    plt.savefig(\"correlation_heatmap.png\")\n",
    "    plt.show()\n",
    "df=None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T22:11:59.548689Z",
     "start_time": "2024-03-18T22:11:59.543726Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb3cfa8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T22:12:00.013338Z",
     "start_time": "2024-03-18T22:12:00.008911Z"
    }
   },
   "outputs": [
    {
     "ename": "StopExecution",
     "evalue": "",
     "output_type": "error",
     "traceback": null
    }
   ],
   "source": [
    "if data_file_path is None:\n",
    "    mr.Stop()\n",
    "else:\n",
    "    df = pd.read_csv(data_file_path)\n",
    "    exploratory_data_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T22:12:00.573297Z",
     "start_time": "2024-03-18T22:12:00.559773Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b85b4124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T22:12:01.184372Z",
     "start_time": "2024-03-18T22:12:01.158008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "### Missing Value Diagnostics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m mr\u001B[38;5;241m.\u001B[39mMarkdown(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m### Missing Value Diagnostics\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmatplotlib\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minline\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m missing_values_table(\u001B[43mdf\u001B[49m)\n\u001B[1;32m      4\u001B[0m msno\u001B[38;5;241m.\u001B[39mmatrix(df\u001B[38;5;241m.\u001B[39msample(\u001B[38;5;241m250\u001B[39m))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "mr.Markdown(\"### Missing Value Diagnostics\")\n",
    "%matplotlib inline\n",
    "missing_values_table(df)\n",
    "msno.matrix(df.sample(250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202bccd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
